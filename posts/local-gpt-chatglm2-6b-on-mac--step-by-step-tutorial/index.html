<!DOCTYPE html>
<html lang="en-us">
  <head>
    <title>Local GPT: ChatGLM2-6B on Mac -- Step by step tutorial | Richard&#39;s sogni d&#39;oro</title>

    <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">    
<meta name="viewport" content="width=device-width,minimum-scale=1">
<meta name="description" content="Apple has released mps backend ^1^ which boosts Macs that have AMD GPU or M series processor runs an LLM locally. This tutorial gives step-by-step instructions to run the ChatGLM2-6B model on a 16-inch MacBook Pro (2019) with 32G memory and AMD Radeon Pro 5500M 4 GB GPU.
Build the enviroment Install openMP curl -O https://mac.r-project.org/openmp/openmp-12.0.1-darwin20-Release.tar.gz
sudo tar fvxz openmp-12.0.1-darwin20-Release.tar.gz -C /
The contained set of files is the same in all tar balls:">

<META NAME="ROBOTS" CONTENT="INDEX, FOLLOW">

<meta name="generator" content="Hugo 0.146.4">



<link rel="stylesheet" href="/css/style.css">


<script data-ad-client="ca-pub-3246815983951661" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>


  
    
    <link rel="stylesheet" href="https://richard-cokecola.github.io/css/chat.css">
  


<link rel="shortcut icon" href="/images/favicon.ico" type="image/x-icon" />

 
    
<script async src="https://www.googletagmanager.com/gtag/js?id=G-HQLWYE4FVQ"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-HQLWYE4FVQ');
</script>



  
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/katex.min.css" integrity="sha384-RZU/ijkSsFbcmivfdRBQDtwuwVqK7GMOw6IMvKyeWL2K5UAlyp6WonmB8m7Jd0Hn" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/katex.min.js" integrity="sha384-pK1WpvzWVBQiP0/GjnvRxV4mOb0oxFuyRxJlk6vVw146n3egcN5C925NCP7a7BY8" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/contrib/auto-render.min.js" integrity="sha384-vZTG03m+2yp6N6BNi5iM4rW4oIwk5DfcNdFfxkk9ZWpDriOkXX8voJBFrAO7MpVl" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>




  </head>

  <body>
    <nav class="navigation">
	
		<a href="/"> <span class="arrow">‚Üê</span>Home</a>
	
	<a href="/posts">Archive</a>
	<a href="/tags">Tags</a>
	<a href="/about">About</a>

	

	
</nav>


    <main class="main">
      

<section id="single">
    <h1 class="title">Local GPT: ChatGLM2-6B on Mac -- Step by step tutorial</h1>

    <div class="tip">
        <span>
          Jun 27, 2023 19:38
        </span>
        <span class="split">
          ¬∑
        </span>
        <span>
          
            643 words
          
        </span>
        <span class="split">
          ¬∑
        </span>
        <span>
          2 minute read
        </span>
    </div>

    
    
        <aside class="toc">
  <details>
      <summary>Table of Contents
      </summary>
      <div>
          <nav id="TableOfContents">
  <ul>
    <li><a href="#build-the-enviroment">Build the enviroment</a>
      <ul>
        <li><a href="#install-openmp">Install openMP</a></li>
        <li><a href="#install-the-latest-version-of-conda">Install the latest version of Conda</a></li>
        <li><a href="#download-anaconda">Download Anaconda</a></li>
        <li><a href="#install-pytorch-nightly">Install PyTorch-Nightly</a></li>
        <li><a href="#verify">Verify</a></li>
        <li><a href="#install-other-requirements">Install other requirements</a></li>
      </ul>
    </li>
    <li><a href="#download-build--run">Download, build &amp; run</a></li>
    <li><a href="#reference">Reference</a></li>
  </ul>
</nav>
      </div>
  </details>
</aside>
    


    <div class="content">
      <p>Apple has released mps backend ^1^ which boosts Macs that have AMD GPU or M series processor runs an LLM locally. This tutorial gives step-by-step instructions to run the ChatGLM2-6B model on a 16-inch MacBook Pro (2019) with 32G memory and AMD Radeon Pro 5500M 4 GB GPU.</p>
<h2 id="build-the-enviroment">Build the enviroment <a href="#build-the-enviroment" class="anchor"></a></h2><h3 id="install-openmp">Install openMP <a href="#install-openmp" class="anchor"></a></h3><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>curl -O https://mac.r-project.org/openmp/openmp-12.0.1-darwin20-Release.tar.gz
</span></span><span style="display:flex;"><span>sudo tar fvxz openmp-12.0.1-darwin20-Release.tar.gz -C /
</span></span></code></pre></div><p>The contained set of files is the same in all tar balls:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>usr/local/lib/libomp.dylib
</span></span><span style="display:flex;"><span>usr/local/include/ompt.h
</span></span><span style="display:flex;"><span>usr/local/include/omp.h
</span></span><span style="display:flex;"><span>usr/local/include/omp-tools.h
</span></span></code></pre></div><h3 id="install-the-latest-version-of-conda">Install the latest version of Conda <a href="#install-the-latest-version-of-conda" class="anchor"></a></h3><p>You can use either Anaconda or pip. Please note that environment setup will differ between a Mac with Apple silicon and a Mac with Intel x86.</p>
<p>Use the PyTorch installation selector <a href="https://pytorch.org/get-started/locally/" target="_blank" rel="noopener">on the installation page</a> to choose Preview (Nightly) for MPS device acceleration. The MPS backend support is part of the PyTorch 1.12 official release. The Preview (Nightly) build of PyTorch will provide the latest <code>mps</code> support on your device.</p>
<h3 id="download-anaconda">Download Anaconda <a href="#download-anaconda" class="anchor"></a></h3><p><strong>For Apple silicon:</strong></p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-MacOSX-arm64.sh
</span></span><span style="display:flex;"><span>sh Miniconda3-latest-MacOSX-arm64.sh
</span></span></code></pre></div><p><strong>For x86:</strong></p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-MacOSX-x86_64.sh
</span></span><span style="display:flex;"><span>sh Miniconda3-latest-MacOSX-x86_64.sh
</span></span></code></pre></div><p>You can use preinstalled <code>pip3</code>, which comes with macOS. Alternatively, you can install it from the Python website or the Homebrew package manager.</p>
<h3 id="install-pytorch-nightly">Install PyTorch-Nightly <a href="#install-pytorch-nightly" class="anchor"></a></h3><p>Before install all the dependencies, create a new conda env:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#080;font-style:italic"># create the env named llm</span>
</span></span><span style="display:flex;"><span>conda create <span style="color:#666">--</span>name llm
</span></span><span style="display:flex;"><span><span style="color:#080;font-style:italic"># enter the env that just created</span>
</span></span><span style="display:flex;"><span>conda activate llm
</span></span></code></pre></div><p>Now we are in the env, start installing. You can choose to use conda or pip.</p>
<p><strong>Using conda:</strong></p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>conda install pytorch torchvision torchaudio -c pytorch-nightly
</span></span></code></pre></div><p><strong>Or using pip:</strong></p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>pip3 install --pre torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/nightly/cpu
</span></span></code></pre></div><p><strong>Or building from source:</strong></p>
<p>Building PyTorch with MPS support requires Xcode 13.3.1 or later. You can download the latest public Xcode release on the Mac App Store or the latest beta release on the <a href="https://apps.apple.com/us/app/xcode/id497799835?mt=12" target="_blank" rel="noopener">Mac App Store</a> or the latest beta release on the <a href="https://developer.apple.com/download/applications/" target="_blank" rel="noopener">Apple Developer website</a>. The <code>USE_MPS</code> environment variable controls building PyTorch and includes MPS support.</p>
<p>To build PyTorch, follow the instructions provided on the <a href="https://github.com/pytorch/pytorch#from-source" target="_blank" rel="noopener">PyTorch website</a>.</p>
<h3 id="verify">Verify <a href="#verify" class="anchor"></a></h3><p>You can verify mps support using a simple Python script:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#a2f;font-weight:bold">import</span> <span style="color:#00f;font-weight:bold">torch</span>
</span></span><span style="display:flex;"><span><span style="color:#a2f;font-weight:bold">if</span> torch<span style="color:#666">.</span>backends<span style="color:#666">.</span>mps<span style="color:#666">.</span>is_available():
</span></span><span style="display:flex;"><span>    mps_device <span style="color:#666">=</span> torch<span style="color:#666">.</span>device(<span style="color:#b44">&#34;mps&#34;</span>)
</span></span><span style="display:flex;"><span>    x <span style="color:#666">=</span> torch<span style="color:#666">.</span>ones(<span style="color:#666">1</span>, device<span style="color:#666">=</span>mps_device)
</span></span><span style="display:flex;"><span>    <span style="color:#a2f">print</span> (x)
</span></span><span style="display:flex;"><span><span style="color:#a2f;font-weight:bold">else</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#a2f">print</span> (<span style="color:#b44">&#34;MPS device not found.&#34;</span>)
</span></span></code></pre></div><p>The output should show:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>tensor([<span style="color:#666">1.</span>], device<span style="color:#666">=</span><span style="color:#b44">&#39;mps:0&#39;</span>)
</span></span></code></pre></div><h3 id="install-other-requirements">Install other requirements <a href="#install-other-requirements" class="anchor"></a></h3><div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>pip install protobuf transformers<span style="color:#666">==</span><span style="color:#666">4.30.2</span> cpm_kernels gradio mdtex2html sentencepiece accelerate
</span></span></code></pre></div><h2 id="download-build--run">Download, build &amp; run <a href="#download-build--run" class="anchor"></a></h2><p><a href="https://huggingface.co/Xorbits/chatglm2-6B-GGML/tree/main" target="_blank" rel="noopener">Download model files</a> first. The model files are:</p>
<blockquote>
<ul>
<li>‚Äã	<code>q4_0</code>: 4-bit integer quantization with fp16 scales.</li>
<li><code>q4_1</code>: 4-bit integer quantization with fp16 scales and minimum values.</li>
<li><code>q5_0</code>: 5-bit integer quantization with fp16 scales.</li>
<li><code>q5_1</code>: 5-bit integer quantization with fp16 scales and minimum values.</li>
<li><code>q8_0</code>: 8-bit integer quantization with fp16 scales.</li>
<li><code>f16</code>: half precision floating point weights without quantization.</li>
<li><code>f32</code>: single precision floating point weights without quantization.</li>
</ul></blockquote>
<p>Then download chatglm.cpp model by using:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>git clone --recursive https://github.com/li-plus/chatglm.cpp.git <span style="color:#666">&amp;&amp;</span> <span style="color:#a2f">cd</span> chatglm.cpp
</span></span></code></pre></div><p><strong>Build and run:</strong></p>
<p>If you have a CUDA GPU, you should use:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>cmake -B build -DGGML_CUBLAS<span style="color:#666">=</span>ON <span style="color:#666">&amp;&amp;</span> cmake --build build -j
</span></span></code></pre></div><p>On Mac, you can use:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>cmake -B build -DGGML_METAL<span style="color:#666">=</span>ON <span style="color:#666">&amp;&amp;</span> cmake --build build -j
</span></span></code></pre></div><p>Only use the CPU:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>cmake -B build
</span></span><span style="display:flex;"><span>cmake --build build -j --config Release
</span></span></code></pre></div><p><strong>Then you are ready to try:</strong></p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#080;font-style:italic"># replace the &#39;chatglm-ggml.bin&#39; to the file you use</span>
</span></span><span style="display:flex;"><span><span style="color:#666">./</span>build<span style="color:#666">/</span><span style="color:#a2f">bin</span><span style="color:#666">/</span>main <span style="color:#666">-</span>m chatglm<span style="color:#666">-</span>ggml<span style="color:#666">.</span>bin <span style="color:#666">-</span>p ‰Ω†Â•Ω
</span></span><span style="display:flex;"><span><span style="color:#080;font-style:italic"># ‰Ω†Â•ΩüëãÔºÅÊàëÊòØ‰∫∫Â∑•Êô∫ËÉΩÂä©Êâã ChatGLM-6BÔºåÂæàÈ´òÂÖ¥ËßÅÂà∞‰Ω†ÔºåÊ¨¢ËøéÈóÆÊàë‰ªª‰ΩïÈóÆÈ¢ò„ÄÇ</span>
</span></span></code></pre></div><p>Test the model in English:
<p class="markdown-image">
  <img src="https://raw.githubusercontent.com/Richard-CokeCola/Richard-CokeCola.github.io/main/images/llm2.jpg" alt="llm"  title="LLM on Mac" />
</p></p>
<p>Input the same question under the MPS backend to enable my GPU in the task:</p>
<p><p class="markdown-image">
  <img src="https://raw.githubusercontent.com/Richard-CokeCola/Richard-CokeCola.github.io/main/images/llm3.jpg" alt="llm"  title="LLM on Mac" />
</p></p>
<p>It shows that MPS backend&rsquo;s prompt time is faster than only use CPU. However, MPS backend&rsquo;s output time is slower than only use CPU.</p>
<p>Test the model&rsquo;s Chinese capability:
<p class="markdown-image">
  <img src="https://raw.githubusercontent.com/Richard-CokeCola/Richard-CokeCola.github.io/main/images/llm.jpg" alt="llm"  title="LLM on Mac" />
</p></p>
<h2 id="reference">Reference <a href="#reference" class="anchor"></a></h2><ol>
<li><a href="https://huggingface.co/docs/accelerate/usage_guides/mps" target="_blank" rel="noopener">What is MPS backend?</a></li>
<li><a href="https://mac.r-project.org/openmp/" target="_blank" rel="noopener">OpenMP on macOS with Xcode tools</a></li>
<li><a href="https://developer.apple.com/metal/pytorch/" target="_blank" rel="noopener">Accelerated PyTorch training on Mac</a></li>
<li><a href="https://github.com/THUDM/ChatGLM2-6B" target="_blank" rel="noopener">ChatGLM2-6B Github page</a></li>
<li><a href="https://github.com/oobabooga/text-generation-webui" target="_blank" rel="noopener">text-generation-webui Github page</a></li>
<li><a href="https://github.com/li-plus/chatglm.cpp" target="_blank" rel="noopener">https://github.com/li-plus/chatglm.cpp</a></li>
</ol>

    </div>

    
        <div class="tags">
            
                <a href="https://richard-cokecola.github.io/tags/tutorial"> tutorial </a>
            
                <a href="https://richard-cokecola.github.io/tags/ai">AI</a>
            
        </div>
    
    
    
  <div id="comment">
    
    <div id="disqus_thread"></div>
<script>
    window.disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "https-richard-cokecola-github-io" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
  </div>


</section>


    </main>
    
    <footer id="footer">
    

    <div class="copyright">
    
        An engineer who is interested in AI, music, cooking, and some other things. You can contact me via email: <a href="mailto:richardwei@protonmail.com">richardwei@protonmail.com</a>
    
    </div>

    
      <div class="powerby">
        Site by <a href="https://richard-cokecola.github.io">Richard</a> Theme By <a href="https://github.com/nodejh/hugo-theme-cactus-plus">nodejh</a>
      </div>
    
</footer>



  </body>
</html>
